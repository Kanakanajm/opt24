\documentclass{article}

% Packages forked from the original template
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

% Extra packages
\usepackage{changepage} % adjustwidth environment
\usepackage{hyperref} % href
\usepackage{xcolor} % colored text

\usetikzlibrary{automata,positioning}

% Additional commands
\def\eg{\emph{e.g., }}
\def\ie{\emph{i.e., }}
\def\cf{\emph{c.f., }}
\def\etc{\emph{etc. }}
\def\wrt{\emph{w.r.t. }}
\def\etal{\emph{et al. }}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Exercise \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Exercise \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Exercise \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Assignment 3}
\newcommand{\hmwkDueDate}{May 14, 2024}
\newcommand{\hmwkClass}{Continuous Optimization}
\newcommand{\hmwkAuthorName}{ \textbf{Honglu Ma} \and \textbf{Hiroyasu Akada} \and \textbf{Mathivathana Ayyappan}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% Norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Margined Homework Subsection
\newenvironment{homeworkSubsection}[1]{%
    \subsection*{#1}%
    \begin{adjustwidth}{2.5em}{0pt}%
}{%
    \end{adjustwidth}%
}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}[1]
\end{homeworkProblem}
\begin{homeworkProblem}[2]
    \begin{homeworkSubsection}{(a)}
        Since $Q$ is a square matrix, we can write $Q = U \Lambda U^T$ where $U$ is an orthogonal matrix 
        and $\Lambda$ is a diagonal matrix with the eigenvalues of $Q$ i.e. $\lambda_i$ on the diagonal.
        Then, we have
        \begin{align*}
            \langle x, Qx \rangle 
            &= \langle x, U\Lambda U^\top x \rangle\\
            &= x^\top U\Lambda U^\top x \\
            &= (U^\top x)^\top\Lambda U^\top x\\
            &= (U x)^\top\Lambda (Ux) \tag*{$U=U^\top$}\\
            &= \sum_{i=1}^n \lambda_i (Ux)_i^2\\
            &\leq  \sum_{i=1}^n \lambda_{\max}(Q) (Ux)_i^2 \tag*{$\lambda_i \leq \lambda_{\max}(Q)$}\\
            &= \lambda_{\max}(Q) \sum_{i=1}^n (Ux)_i^2\\
            &= \lambda_{\max}(Q) (Ux)^\top Ux\\
            &= \lambda_{\max}(Q) x^\top U^\top Ux\\
            &= \lambda_{\max}(Q) x^\top x \tag*{$U^\top U = I$}\\
            &= \lambda_{\max}(Q) ||x||^2\\
        \end{align*}
        Similar derivation can be shown for the smallest eigenvalue: $\langle x, Qx \rangle  \geq \lambda_{\min}(Q)||x||^2$.
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        Suppose $\lambda$ is an eigenvalue of $Q$ with eigenvector $v$. Then, we have
        \begin{align*}
            Qv = \lambda v
            &\Rightarrow \tau Qv = \tau \lambda v\\
            &\Rightarrow Iv - \tau Qv = Iv - \tau\lambda v\\
            &\Rightarrow (I - \tau Q)v = (I - \tau \mathrm{diag}(\lambda)) v\\
            \intertext{$I - \tau \mathrm{diag}(\lambda)$ is a matrix with same diagonal entries $1 - \tau \lambda$}\\
            &\Rightarrow (I - \tau Q)v = (1-\tau \lambda)v\\
            \intertext{Above shows that $1-\tau \lambda$ is an eigenvalue of $I - \tau Q$.}\\
            &\Rightarrow (I - \tau Q)(I - \tau Q)v = (I - \tau Q)(1-\tau \lambda)v\\
            &\Rightarrow (I - \tau Q)(I - \tau Q)v = (1-\tau \lambda)(I - \tau Q)v\\
            &\Rightarrow (I - \tau Q)(I - \tau Q)v = (1-\tau \lambda)(1-\tau \lambda)v\\
            &\Rightarrow (I - \tau Q)^2v = (1-\tau \lambda)^2v\\
        \end{align*}
        Thus $(1-\tau \lambda)^2$ is an eigenvalue of $(I - \tau Q)^2$ for each eigenvalue $\lambda$ of $Q$.
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}[3]
    \begin{homeworkSubsection}{(a)}
        Projection of $\mathbf{v}$ onto the space spanned by the columns of $\mathbf{A}$
        means that we want to find a point in the column space of $\mathbf{A}$ that is closest to $\mathbf{v}$
        i.e. for vector $\mathbf{p}$ in the column space we want the distance between $\mathbf{v}$ and $\mathbf{p}$ to be as small as possible
        \[
            \underset{p}{\mathrm{argmin}}\;\mathrm{dist}\,(\mathbf{v}, \mathbf{p})
        \]
        which is equivalent to
        \[
            \underset{p}{\mathrm{argmin}}\;||\mathbf{v} - \mathbf{p}||
        \]
        or
        \[
            \underset{p}{\mathrm{argmin}}\;\sqrt{\langle\mathbf{v} - \mathbf{p}, \mathbf{v} - \mathbf{p}\rangle}
        \]
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        In case of $m = 1$, the subspace spanned by $\mathbf{u}$ is just a line $c\mathbf{u}$ for some constant $c \in \mathbb{R}$. 
        We have
        \[
            \underset{c}{\mathrm{argmin}}\;\sqrt{\langle\mathbf{v} - c\mathbf{u}, \mathbf{v} - c\mathbf{u}\rangle}
        \]
        which by definition of the inner product, is equivalent to
        \[
            \underset{c}{\mathrm{argmin}}\;(\mathbf{v} - c\mathbf{u})^\top \mathbf{Q}(\mathbf{v} - c\mathbf{u})
        \]
        To find minimum, we take derivative of function 
        \[
            f(c) = (\mathbf{v} - c\mathbf{u})^\top \mathbf{Q}(\mathbf{v} - c\mathbf{u}) 
            = \mathbf{v}^\top \mathbf{Q}\mathbf{v} - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u})c + (\mathbf{u}^\top \mathbf{Q}\mathbf{u})c^2 
        \]
        We have
        \[
            f'(c) = 2(\mathbf{u}^\top \mathbf{Q}\mathbf{u})c - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u})
        \]
        and the second derivative
        \[
            f''(c) = 2(\mathbf{u}^\top \mathbf{Q}\mathbf{u}) \geq 0
        \]
        which indicate function $f$ is convex now we just have to solve $f'(c) = 0$
        \begin{align*}
            2(\mathbf{u}^\top \mathbf{Q}\mathbf{u})c - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u}) = 0&\\
            c = \frac{\mathbf{v}^\top \mathbf{Q}\mathbf{u}}{\mathbf{u}^\top \mathbf{Q}\mathbf{u}}&\\
        \end{align*}
        and the projection $\mathbf{p} = \frac{\mathbf{v}^\top \mathbf{Q}\mathbf{u}}{\mathbf{u}^\top \mathbf{Q}\mathbf{u}}\mathbf{u}$
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(c)}
        In case of $m > 1$, we have a vector $\mathbf{\hat{c}} \in \mathbb{R}^m$ defined by
        \[
            \mathbf{\hat{c}} := \begin{pmatrix}
                c_1\\
                \vdots\\
                c_m\\
            \end{pmatrix}
        \]
        where the projection $\mathbf{p} = \mathbf{A}\mathbf{\hat{c}}$. We have
        \[
            \underset{\mathbf{\hat{c}}}{\mathrm{argmin}}\;(\mathbf{v} - \mathbf{A}\mathbf{\hat{c}})^\top \mathbf{Q}(\mathbf{v} - \mathbf{A}\mathbf{\hat{c}})
        \]
        To find the minimum, similarly, we define a function 
        \[
            g(\mathbf{\hat{c}}) = \mathbf{v}^\top \mathbf{Q}\mathbf{v} - 2\mathbf{v}^\top \mathbf{Q}\mathbf{A}\mathbf{\hat{c}} + \mathbf{\hat{c}}^\top\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} 
        \]
        We calculate its gradient
        \[
            \nabla g(\mathbf{\hat{c}}) = -2\mathbf{v}^\top \mathbf{Q}\mathbf{A} + 2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}}
        \]
        and its Hessian
        \[
            \nabla^2 g(\mathbf{\hat{c}}) = 2\mathbf{A}^\top\mathbf{Q}\mathbf{A} = \mathbf{H}
        \]
        We can show that $\mathbf{H}$ is positive semi-definite. 
        Suppose $\lambda$ is an eigenvalue of $\mathbf{H}$ 
        and with corresponding eigenvector $\mathbf{e}$
        \begin{align*}
            \mathbf{H}\mathbf{e} = \lambda\mathbf{e}&\\
            2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{e} = \lambda\mathbf{e}&\\
            2\mathbf{e}^\top\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{e} = \lambda\mathbf{e}^\top\mathbf{e}&\\
            \lambda = \frac{2(\mathbf{A}\mathbf{e})^\top\mathbf{Q}\mathbf{A}\mathbf{e}}{\mathbf{e}^\top\mathbf{e}} \geq 0&\\
        \end{align*}
        Thus we can get the minimizer $\mathbf{\hat{c}}$ by solving $\nabla g(\mathbf{\hat{c}}) = 0$
        \begin{align*}
            -2\mathbf{v}^\top \mathbf{Q}\mathbf{A} + 2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} = 0&\\
            \mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} = \mathbf{v}^\top \mathbf{Q}\mathbf{A}&\\
        \end{align*}
    \end{homeworkSubsection}
\end{homeworkProblem}

\end{document}
