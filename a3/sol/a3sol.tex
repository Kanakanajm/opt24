\documentclass{article}

% Packages forked from the original template
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

% Extra packages
\usepackage{changepage} % adjustwidth environment
\usepackage{hyperref} % href
\usepackage{xcolor} % colored text

\usetikzlibrary{automata,positioning}

% Additional commands
\def\eg{\emph{e.g., }}
\def\ie{\emph{i.e., }}
\def\cf{\emph{c.f., }}
\def\etc{\emph{etc. }}
\def\wrt{\emph{w.r.t. }}
\def\etal{\emph{et al. }}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Exercise \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Exercise \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Exercise \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Assignment 3}
\newcommand{\hmwkDueDate}{May 14, 2024}
\newcommand{\hmwkClass}{Continuous Optimization}
\newcommand{\hmwkAuthorName}{ \textbf{Honglu Ma} \and \textbf{Hiroyasu Akada} \and \textbf{Mathivathana Ayyappan}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% Norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Margined Homework Subsection
\newenvironment{homeworkSubsection}[1]{%
    \subsection*{#1}%
    \begin{adjustwidth}{2.5em}{0pt}%
}{%
    \end{adjustwidth}%
}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}[1]
    \begin{enumerate}
        \item $f(x) - f(\bar{x}) = \int_{0}^{1}\langle\nabla f(\textcolor{red}{x}+ t(x - \bar{x}), x - \bar{x})\rangle\,dt$
        
        should be $f(x) - f(\bar{x}) = \int_{0}^{1}\langle\nabla f(\bar{x} + t(x - \bar{x}), x - \bar{x})\rangle\,dt$
        \item $\int_{0}^{1}\langle\nabla f(\bar{x} + t(x - \bar{x}), x - \bar{x})\rangle\,dt \leq \textcolor{red}{\int_{0}^{1}\langle\nabla f(\bar{x}), x - \bar{x}\rangle\,dt} + \hdots$
        
        should be $\int_{0}^{1}\langle\nabla f(\bar{x} + t(x - \bar{x}), x - \bar{x})\rangle\,dt \leq - \int_{0}^{1}\langle\nabla f(\bar{x}), x - \bar{x}\rangle\,dt + \hdots$
        \item (continue from the previous equation) $\hdots \textcolor{red}{|}\int_{0}^{1}\langle\nabla f(\bar{x} + t(x - \bar{x}) - \nabla f(\bar{x}), x - \bar{x})\rangle\,dt\textcolor{red}{|}$
        
        should drop the absolute value sign
        \item The correct equation consists the previous three errors should be
        \begin{align*}
            &f(x) - f(\bar{x}) - \langle\nabla f(\bar{x}), x - \bar{x}\rangle\\
            &= \int_{0}^{1}\langle\nabla f(\bar{x} + t(x - \bar{x}), x - \bar{x}) - \nabla f(\bar{x}), x - \bar{x}\rangle\,dt\\
            &\leq \int_{0}^{1}||\nabla f(\bar{x} + t(x - \bar{x})) - \nabla f(\bar{x})||\,||x - \bar{x}||\,dt \tag*{Cauchy Schwarz Inequality}
        \end{align*}
        \item Continue with previous derivation we have
        \begin{align*}
            &\int_{0}^{1}||\nabla f(\bar{x} + t(x - \bar{x})) - \nabla f(\bar{x})||\,||x - \bar{x}||\,dt\\
            &\leq \int_{0}^{1}L||\bar{x} + t(x - \bar{x}) - \bar{x}||\,||x - \bar{x}||\,dt \tag*{Lipschitz Continuous Gradient}\\
            &= \int_{0}^{1}L||t(x - \bar{x})||\,||x - \bar{x}||\,dt\\
            &= L||x - \bar{x}||^2\int_{0}^{1}t\,dt\\
            &= L||x - \bar{x}||^2 (\frac{t^2}{2}\rvert_{0}^{1})\\
            &= \frac{L}{2}||x - \bar{x}||^2
        \end{align*}
        where in the original proof, there is an error at $\hdots \int_{0}^{1}\textcolor{red}{\frac{Lt}{2}}\hdots$
        \item[6-10.] There are serveral errors in the last part of the proof. In addition to adding a half to $Lt$.
        The derivation also does not follow the correct steps of integration.
    \end{enumerate}
\end{homeworkProblem}
\begin{homeworkProblem}[2]
    \begin{homeworkSubsection}{(a)}
        Since $\mathbf{Q}$ is a square matrix, we can write $\mathbf{Q} = \mathbf{U} \Lambda \mathbf{U}^\top$ where $\mathbf{U}$ is an orthogonal matrix 
        and $\Lambda$ is a diagonal matrix with the eigenvalues of $\mathbf{Q}$ i.e. $\lambda_i$ on the diagonal.
        Then, we have
        \begin{align*}
            \langle \mathbf{x}, \mathbf{Q}\mathbf{x} \rangle 
            &= \langle \mathbf{x}, \mathbf{U}\Lambda \mathbf{U}^\top \mathbf{x} \rangle\\
            &= \mathbf{x}^\top \mathbf{U}\Lambda \mathbf{U}^\top \mathbf{x} \\
            &= (\mathbf{U}^\top \mathbf{x})^\top\Lambda \mathbf{U}^\top \mathbf{x}\\
            &= (\mathbf{U} \mathbf{x})^\top\Lambda (\mathbf{U}\mathbf{x}) \tag*{$\mathbf{U}=\mathbf{U}^\top$}\\
            &= \sum_{i=1}^n \lambda_i ((\mathbf{U}\mathbf{x})_i)^2\\
            &\leq  \sum_{i=1}^n \lambda_{\max}(\mathbf{Q}) ((\mathbf{U}\mathbf{x})_i)^2 \tag*{$\lambda_i \leq \lambda_{\max}(\mathbf{Q})$}\\
            &= \lambda_{\max}(\mathbf{Q}) \sum_{i=1}^n ((\mathbf{U}\mathbf{x})_i)^2\\
            &= \lambda_{\max}(\mathbf{Q}) (\mathbf{U}\mathbf{x})^\top \mathbf{U}\mathbf{x}\\
            &= \lambda_{\max}(\mathbf{Q}) \mathbf{x}^\top \mathbf{U}^\top \mathbf{U}\mathbf{x}\\
            &= \lambda_{\max}(\mathbf{Q}) \mathbf{x}^\top \mathbf{x} \tag*{$\mathbf{U}^\top \mathbf{U} = I$}\\
            &= \lambda_{\max}(\mathbf{Q}) ||\mathbf{x}||^2\\
        \end{align*}
        Similar derivation can be shown for the smallest eigenvalue: $\langle \mathbf{x}, \mathbf{Q}\mathbf{x} \rangle  \geq \lambda_{\min}(\mathbf{Q})||\mathbf{x}||^2$.
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        Suppose $\lambda$ is an eigenvalue of $\mathbf{Q}$ with eigenvector $\mathbf{v}$. Then, we have
        \begin{align*}
            \mathbf{Q}\mathbf{v} = \lambda \mathbf{v}
            &\Rightarrow \tau \mathbf{Q}\mathbf{v} = \tau \lambda \mathbf{v}\\
            &\Rightarrow \mathbf{I}\mathbf{v} - \tau \mathbf{Q}\mathbf{v} = \mathbf{I}\mathbf{v} - \tau\lambda \mathbf{v}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})\mathbf{v} = (\mathbf{I} - \tau \mathrm{diag}(\lambda)) \mathbf{v}\\
            \intertext{$\mathbf{I} - \tau \mathrm{diag}(\lambda)$ is a matrix with same diagonal entries $1 - \tau \lambda$}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})\mathbf{v} = (1-\tau \lambda)\mathbf{v}\\
            \intertext{Above shows that $1-\tau \lambda$ is an eigenvalue of $\mathbf{I} - \tau \mathbf{Q}$.}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})(\mathbf{I} - \tau \mathbf{Q})\mathbf{v} = (\mathbf{I} - \tau \mathbf{Q})(1-\tau \lambda)\mathbf{v}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})(\mathbf{I} - \tau \mathbf{Q})\mathbf{v} = (1-\tau \lambda)(\mathbf{I} - \tau \mathbf{Q})\mathbf{v}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})(\mathbf{I} - \tau \mathbf{Q})\mathbf{v} = (1-\tau \lambda)(1-\tau \lambda)\mathbf{v}\\
            &\Rightarrow (\mathbf{I} - \tau \mathbf{Q})^2\mathbf{v} = (1-\tau \lambda)^2\mathbf{v}\\
        \end{align*}
        Thus $(1-\tau \lambda)^2$ is an eigenvalue of $(\mathbf{I} - \tau \mathbf{Q})^2$ for each eigenvalue $\lambda$ of $\mathbf{Q}$.
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}[3]
    \begin{homeworkSubsection}{(a)}
        Projection of $\mathbf{v}$ onto the space spanned by the columns of $\mathbf{A}$
        means that we want to find a point in the column space of $\mathbf{A}$ that is closest to $\mathbf{v}$
        i.e. for vector $\mathbf{p}$ in the column space we want the distance between $\mathbf{v}$ and $\mathbf{p}$ to be as small as possible
        \[
            \underset{p}{\mathrm{argmin}}\;\mathrm{dist}\,(\mathbf{v}, \mathbf{p})
        \]
        which is equivalent to
        \[
            \underset{p}{\mathrm{argmin}}\;||\mathbf{v} - \mathbf{p}||
        \]
        or
        \[
            \underset{p}{\mathrm{argmin}}\;\sqrt{\langle\mathbf{v} - \mathbf{p}, \mathbf{v} - \mathbf{p}\rangle}
        \]
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        In case of $m = 1$, the subspace spanned by $\mathbf{u}$ is just a line $c\mathbf{u}$ for some constant $c \in \mathbb{R}$. 
        We have
        \[
            \underset{c}{\mathrm{argmin}}\;\sqrt{\langle\mathbf{v} - c\mathbf{u}, \mathbf{v} - c\mathbf{u}\rangle}
        \]
        which by definition of the inner product, is equivalent to
        \[
            \underset{c}{\mathrm{argmin}}\;(\mathbf{v} - c\mathbf{u})^\top \mathbf{Q}(\mathbf{v} - c\mathbf{u})
        \]
        To find minimum, we take derivative of function 
        \[
            f(c) = (\mathbf{v} - c\mathbf{u})^\top \mathbf{Q}(\mathbf{v} - c\mathbf{u}) 
            = \mathbf{v}^\top \mathbf{Q}\mathbf{v} - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u})c + (\mathbf{u}^\top \mathbf{Q}\mathbf{u})c^2 
        \]
        We have
        \[
            f'(c) = 2(\mathbf{u}^\top \mathbf{Q}\mathbf{u})c - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u})
        \]
        and the second derivative
        \[
            f''(c) = 2(\mathbf{u}^\top \mathbf{Q}\mathbf{u}) \geq 0
        \]
        which indicate function $f$ is convex now we just have to solve $f'(c) = 0$
        \begin{align*}
            2(\mathbf{u}^\top \mathbf{Q}\mathbf{u})c - 2(\mathbf{v}^\top \mathbf{Q}\mathbf{u}) = 0&\\
            c = \frac{\mathbf{v}^\top \mathbf{Q}\mathbf{u}}{\mathbf{u}^\top \mathbf{Q}\mathbf{u}}&\\
        \end{align*}
        and the projection $\mathbf{p} = \frac{\mathbf{v}^\top \mathbf{Q}\mathbf{u}}{\mathbf{u}^\top \mathbf{Q}\mathbf{u}}\mathbf{u}$
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(c)}
        In case of $m > 1$, we have a vector $\mathbf{\hat{c}} \in \mathbb{R}^m$ defined by
        \[
            \mathbf{\hat{c}} := \begin{pmatrix}
                c_1\\
                \vdots\\
                c_m\\
            \end{pmatrix}
        \]
        where the projection $\mathbf{p} = \mathbf{A}\mathbf{\hat{c}}$. We have
        \[
            \underset{\mathbf{\hat{c}}}{\mathrm{argmin}}\;(\mathbf{v} - \mathbf{A}\mathbf{\hat{c}})^\top \mathbf{Q}(\mathbf{v} - \mathbf{A}\mathbf{\hat{c}})
        \]
        To find the minimum, similarly, we define a function 
        \[
            g(\mathbf{\hat{c}}) = \mathbf{v}^\top \mathbf{Q}\mathbf{v} - 2\mathbf{v}^\top \mathbf{Q}\mathbf{A}\mathbf{\hat{c}} + \mathbf{\hat{c}}^\top\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} 
        \]
        We calculate its gradient
        \[
            \nabla g(\mathbf{\hat{c}}) = -2\mathbf{v}^\top \mathbf{Q}\mathbf{A} + 2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}}
        \]
        and its Hessian
        \[
            \nabla^2 g(\mathbf{\hat{c}}) = 2\mathbf{A}^\top\mathbf{Q}\mathbf{A} = \mathbf{H}
        \]
        We can show that $\mathbf{H}$ is positive semi-definite. 
        Suppose $\lambda$ is an eigenvalue of $\mathbf{H}$ 
        and with corresponding eigenvector $\mathbf{e}$
        \begin{align*}
            \mathbf{H}\mathbf{e} = \lambda\mathbf{e}&\\
            2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{e} = \lambda\mathbf{e}&\\
            2\mathbf{e}^\top\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{e} = \lambda\mathbf{e}^\top\mathbf{e}&\\
            \lambda = \frac{2(\mathbf{A}\mathbf{e})^\top\mathbf{Q}\mathbf{A}\mathbf{e}}{\mathbf{e}^\top\mathbf{e}} \geq 0&\\
        \end{align*}
        Thus we can get the minimizer $\mathbf{\hat{c}}$ by solving $\nabla g(\mathbf{\hat{c}}) = 0$
        \begin{align*}
            -2\mathbf{v}^\top \mathbf{Q}\mathbf{A} + 2\mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} = 0&\\
            \mathbf{A}^\top\mathbf{Q}\mathbf{A}\mathbf{\hat{c}} = \mathbf{v}^\top \mathbf{Q}\mathbf{A}&\\
        \end{align*}
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}
    \begin{homeworkSubsection}{(a)}
        Define $\tilde{A}$ and $\tilde{f}$ accordingly:
        \[
            \tilde{A} = \begin{pmatrix}
                A\\
                \sqrt{\mu}D
            \end{pmatrix},\quad
            \tilde{f} = \begin{pmatrix}
                f\\
                0
            \end{pmatrix}
            \tag*{Note the $0$ in $\tilde{f}$ denotes a zero vector in $\mathbb{R}^{2N}$}
        \]
        where $\tilde{A}\in\mathbb{R}^{3N\times N}$ and $\tilde{f}\in\mathbb{R}^{3N}$.

        We can verify that the reconstruction is equivalent to the original problem.
        \begin{align*}
            \frac{1}{2}||\tilde{A}u-\tilde{f}||^2
            &= \frac{1}{2}(||\tilde{A}u||^2 - 2(\tilde{A}u)^\top\tilde{f} + ||\tilde{f}||^2)\\
            &= \frac{1}{2}(||Au||^2 + ||\sqrt{\mu}Du||^2 - 2(Au)^\top f + ||f||^2 + 0)\\
            &= \frac{1}{2}(||Au||^2 + \mu||Du||^2 - 2(Au)^\top f + ||f||^2)\\
            &= \frac{1}{2}(||Au||^2 - 2(Au)^\top f + ||f||^2) + \frac{\mu}{2}||Du||^2\\
            &= \frac{1}{2}||Au - f||^2 + \frac{\mu}{2}||Du||^2\\
        \end{align*}
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        We have the following objective function:
        \begin{align*}
            L(u) &= \frac{1}{2}||\tilde{A}u-\tilde{f}||^2\\
            &= \frac{1}{2}||\tilde{A}u||^2 - \tilde{f}^\top\tilde{A}u + \frac{1}{2}||\tilde{f}||^2\\
            &= \frac{1}{2}\langle u, \tilde{A} u\rangle + \langle -A^\top f, u\rangle + \frac{1}{2}||f||^2\\
        \end{align*}
        with corresponding standard form $L(u) = \frac{1}{2}\langle u, Q u\rangle + \langle b, u\rangle + c$ where
         $Q = \tilde{A}^\top\tilde{A} = A^\top A + \mu D^\top D$, $b =  -A^\top f$ and $c = \frac{1}{2}||f||^2$
        and its gradient:
        \[
            \nabla L(u) = Q u + b = (A^\top A + \mu D^\top D)u - A^\top f
        \]
        The step size for exact line search can be defined as:
        \[
            \tau_k := \frac{\langle -\nabla L(u^{(k)}), b - Qu^{(k)}\rangle}{\langle-\nabla L(u^{(k)}), -Q\nabla L(u^{(k)})\rangle}\]
    \end{homeworkSubsection}
    \end{homeworkProblem}
\end{document}
