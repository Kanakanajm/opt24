\documentclass{article}

% Packages forked from the original template
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

% Extra packages
\usepackage{changepage} % adjustwidth environment
\usepackage{hyperref} % href
\usepackage{xcolor} % colored text
\usepackage{bbm} % bold numbers


\usetikzlibrary{automata,positioning}

% Additional commands
\def\eg{\emph{e.g., }}
\def\ie{\emph{i.e., }}
\def\cf{\emph{c.f., }}
\def\etc{\emph{etc. }}
\def\wrt{\emph{w.r.t. }}
\def\etal{\emph{et al. }}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Exercise \arabic{#1} (continued)}{Exercise \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Exercise \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Exercise \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Exercise \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Assignment 10}
\newcommand{\hmwkDueDate}{July 2, 2024}
\newcommand{\hmwkClass}{Continuous Optimization}
\newcommand{\hmwkAuthorName}{ \textbf{Honglu Ma} \and \textbf{Hiroyasu Akada} \and \textbf{Mathivathana Ayyappan}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% Norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Margined Homework Subsection
\newenvironment{homeworkSubsection}[1]{%
    \subsection*{#1}%
    \begin{adjustwidth}{2.5em}{0pt}%
}{%
    \end{adjustwidth}%
}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}[1]
    We can reformulate the problem as follows:
    \[
        \min_{x \in \mathbb{R}^n} \langle c, x\rangle
        \quad \text{s.t.} \quad
        \forall i \in \{1, \ldots, m\}: b_i - \langle a_i, x\rangle = 0
        \quad \text{and} \quad
        \forall j \in \{1, \ldots, n\}: -x_j \leq 0
    \]
    where $a_i$ is the $i$-th row of $A$, $b_i$ is the $i$-th element of $b$
    and $x_j$ is the $j$-th element of $x$.

    We can name the objective function as $f(x) = \langle c, x\rangle$,
    the equality constraints as $f_i(x) = b_i - \langle a_i, x\rangle,\,\forall i \in \{1, \ldots, m\}$
    and the inequality constraints as $g_j(x) = -x_j,\,\forall j \in \{1, \ldots, n\}$
    such that we have a problem that fits the general form provided in Corollary 15.19, namely:
    \[
        \min_{x \in \mathbb{R}^n} f(x)
        \quad \text{s.t.} \quad
        f_i(x) = 0, i\in\mathcal{E}
        \quad \text{and} \quad
        g_j(x) \leq 0, j\in\mathcal{I}
    \]
    By Corollary 15.19, we know that at optimal $x^\star$, we have
    \begin{align*}
        \nabla f(x^\star) + \sum_{i\in\mathcal{E}} \lambda_i \nabla f_i(x^\star) + \sum_{j\in\mathcal{A}(x^\star)} \mu_j \nabla g_j(x^\star) &= 0\\
        c - \sum_{i=1}^m \lambda_i a_i - \sum_{j\in\mathcal{A}(x^\star)} \mu_j e_j &= 0\\
        c - A^\top\lambda - \mu &= 0
    \end{align*}
    where 
    \[
        \mu = \begin{cases}
            0 & \text{if } x_j^\star > 0\\
            \mu_j > 0 & \text{if } x_j^\star = 0
        \end{cases}
    \]
    observe that $\mu \geq 0$ which is the fouth KKT condition
    and $\mu_j x_j^\star = 0,\,\forall j\in\{1,\ldots,n\}$
    which is the fifth KKT condition a.k.a the complementary condition.
    Also by reformulate the equation we derivate at optimal, we have the first KKT condition:
    \[
        c = A^\top\lambda + \mu
    \]
\end{homeworkProblem}

\begin{homeworkProblem}[2]
    Note that
    \[
        \mathrm{tr}(B^\top X) = \sum_{i=1}^n \langle B^\top_{i,} X_{,i} \rangle
    \]
    where $B^\top_{i,}$ denotes the $i$-th row of $B^\top$
    and $X_{,i}$ denotes the $i$-th column of $X$
    which can also just be seen as a sum of dot products between each column of $B$ and $X$
    i.e. $\sum_{i=1}^n\langle b_i, x_i\rangle$
    where $b_i$ is the $i$-th column of $B$ and $x_i$ is the $i$-th column of $X$.

    To find the minimum of a sum is the same as finding the minimum of each term in the sum
    i.e. instead of $1$ objective function $f(x) = \sum_{i=1}^n\langle b_i, x_i\rangle$,
    we have $n$ objective functions $f_i(x) = \langle b_i, x_i\rangle$
    with same constraints on $x_i$ as such
    \[
        \min_{x_i \in \mathbb{R}^n} \langle b_i, x_i\rangle
        \quad \text{s.t.} \quad
        \sum_{j=1}^n {x_i}_j = 1
        \quad \text{and} \quad
        \forall j \in \{1, \ldots, n\}: {x_i}_j \geq 0
    \]
    for all $i\in\{1,\ldots,n\}$

    The Lagrangian for the $i$-th objective function is (to simplify notation, we will drop the $i$ subscript from now on)
    \begin{align*}
        L(x, \lambda, \mu) &= \langle b, x\rangle - \lambda\left(\sum_{j=1}^n x_j - 1\right) - \sum_{j=1}^n \mu_j x_j\\
        &= \langle b, x\rangle - \lambda\left(\sum_{j=1}^n x_j - 1\right) - \langle \mu, x\rangle
    \end{align*}
    where
    \[ 
        \mu = \begin{cases}
            0 & \text{if } x_j > 0\\
            \mu_j > 0 & \text{if } x_j = 0
        \end{cases}
    \]
    We take the derivative of the Lagrangian with respect to $x$, $\lambda$ and $\mu$ respectively:
    \begin{align*}
        &\nabla_x L(x, \lambda, \mu) = b - \lambda \mathbbm{1} - \mu = 0 \Leftrightarrow \mu_j = b_j - \lambda,\,\forall j\in\{1,\ldots,n\}\\
        &\nabla_\lambda L(x, \lambda, \mu) = -\left(\sum_{j=1}^n x_j - 1\right) = 0 \Leftrightarrow \sum_{j=1}^n x_j = 1\\
        &\nabla_\mu L(x, \lambda, \mu) = -x \leq 0 \Leftrightarrow x \geq 0\\
        &\mu \geq 0\\
        &\mu_j \cdot x_j = 0,\,\forall j\in\{1,\ldots,n\}\\
    \end{align*}
    We have such system of equations ($n+1$ unknowns and $n+1$ equations)
    \begin{align*}
        \mu_j \cdot x_j &= 0\\
        \sum_{j=1}^n x_j &= 1\\
    \end{align*}
    Solve for each column in $X$ and we have the solution to the original problem.
\end{homeworkProblem}
\begin{homeworkProblem}[3]
    Observe that the shape of the constraint set $C$ is like a box (as in $\mathbb{R}^3$)
    \begin{homeworkSubsection}{(a)}
        The Conditional Gradient Method first finds
        \[
            \tilde{x}^{(k)} \in \mathrm{argmin}_{x\in C} \langle \nabla f(x^{(k)}), x - x^{(k)}\rangle
        \]
        and the descend direction is defined as $d^{(k)} = \tilde{x}^{(k)} - x^{(k)}$.
        The time step $\tau_k$ is determined by the backtracking line search that satisfies the Armijo condition with parameter $\gamma$.
        At last, the next point is updated by $x^{(k+1)} = x^{(k)} + \tau_k d^{(k)}$.
    \end{homeworkSubsection}
    \begin{homeworkSubsection}{(b)}
        The Projected Gradient Method first find a point $\bar{x}^{(k)} := x^{(k)} - \alpha\nabla f(x^{(k)})$
        and calculate the its projection onto the feasible set $C$ by $\tilde{x}^{(k)} := \mathrm{proj}_C(\bar{x}^{(k)})$.
        Then the desend direction is the difference between the projected point and the current point
        i.e. $d^{(k)} = \tilde{x}^{(k)} - x^{(k)}$
        and the time step $\tau_k$ is determined by the backtracking line search that satisfies the Armijo condition with paramter $\gamma$.
        At last, the next point is updated by $x^{(k+1)} = x^{(k)} + \tau_k d^{(k)}$.

        The only step that we need to derive is the projection onto the feasible set $C$
        which is defined by the following minimization problem:
        \[
            \tilde{x}^{(k)} = \mathrm{argmin}_{x\in C} \textcolor{red}{\frac{1}{2}}||x - \bar{x}^{(k)}||^2
        \]
        by the optimality condition, we have
        \[
            \tilde{x}^{(k)}  - \bar{x}^{(k)} \in N_C(\tilde{x}^{(k)}),\,\tilde{x}^{(k)} \in C
        \]
        and the projection is given by
        \[
            \tilde{x}^{(k)}_i = \begin{cases}
                q_i & \text{if } \bar{x}^{(k)}_i \geq q_i\\
                p_i & \text{if } \bar{x}^{(k)}_i \leq p_i\\
                \bar{x}^{(k)}_i & \text{otherwise}
            \end{cases}
        \]
    \end{homeworkSubsection}
\end{homeworkProblem}
\begin{homeworkProblem}[4]
    The objective function is given by
    \[ 
        J(u) = \frac{1}{2}||Au - f||^2 + \mu \sum_{i=1}^{N} \sqrt{(Du)_i^2 + (Du)_{i+N}^2 + \epsilon^2}
    \]
    which is the same as
    \[ 
        J(u) = \frac{1}{2}||Au - f||^2 + \mu \sum_{i=1}^{N} \sqrt{\langle D_i, u\rangle^2 + \langle D_{i+N}, u\rangle^2 + \epsilon^2}
    \]
    The gradient of the objective function can be calculated as follows:
    \[
        \nabla J(u) = A^\top(Au - f) + \mu \sum_{i=1}^{N} \frac{(\langle D_i, u\rangle D_i + \langle D_{i+N}, u\rangle D_{i+N})}{\sqrt{\langle D_i, u\rangle^2 + \langle D_{i+N}, u\rangle^2 + \epsilon^2}}
    \]
\end{homeworkProblem}
\end{document}
